Abstrakt fehlt
Wir betrachten eine Klasse von sparsamkeitsinduzierenden Regularisierungsbegriffen, die auf submodularen Funktionen basieren. Während frühere Arbeiten sich auf nicht abnehmende Funktionen konzentriert haben, untersuchen wir symmetrische submodulare Funktionen und ihre \Lova-Erweiterungen. Wir zeigen, dass die Lovasz-Erweiterung als die konvexe Hülle einer Funktion gesehen werden kann, die von Niveausätzen abhängt (d.h. dem Satz von Indizes, deren entsprechende Komponenten des zugrunde liegenden Prädiktors größer als eine gegebene Konstante sind): Dies führt zu einer Klasse von konvex strukturierten Regularisierungstermen, die den Niveausätzen und nicht nur den Trägern der zugrunde liegenden Prädiktoren Vorwissen auferlegen. Wir bieten einen einheitlichen Satz von Optimierungsalgorithmen, wie z.B. proximale Operatoren, und theoretische Garantien (erlaubte Niveausätze und Wiederherstellungsbedingungen). Durch die Auswahl spezifischer submodularer Funktionen geben wir eine neue Interpretation bekannter Normen, wie z.B. der Gesamtvariation; wir definieren auch neue Normen, insbesondere solche, die auf Ordnungsstatistiken mit Anwendung auf Clustering und Ausreißererkennung sowie auf verrauschten Schnitten in Graphen mit Anwendung auf Änderungspunkterkennung bei Vorhandensein von Ausreißern basieren.
Wir stellen einen probabilistischen Algorithmus für nichtlineares inverses Verstärkungslernen vor. Das Ziel des inversen Verstärkungslernens ist es, die Belohnungsfunktion in einem Markov-Entscheidungsprozess aus Expertendemonstrationen zu lernen. Während die meisten früheren Algorithmen des inversen Verstärkungslernens die Belohnung als eine lineare Kombination einer Menge von Merkmalen darstellen, verwenden wir Gauß'sche Verfahren, um die Belohnung als nichtlineare Funktion zu lernen und gleichzeitig die Relevanz jedes Merkmals für die Politik des Experten zu bestimmen. Unser probabilistischer Algorithmus ermöglicht es, komplexe Verhaltensweisen aus suboptimalen stochastischen Demonstrationen zu erfassen und dabei automatisch die Einfachheit der gelernten Belohnungsstruktur gegen ihre Konsistenz mit den beobachteten Aktionen abzuwägen.
Wir stellen ein neuartiges aktives Lern-Framework für Video-Annotation vor. Durch die wohlüberlegte Auswahl der Frames, die ein Benutzer annotieren sollte, können wir mit minimalem Benutzeraufwand hochpräzise Spuren erhalten.  Wir betrachten dieses Problem als ein Problem des aktiven Lernens und zeigen, dass wir eine ausgezeichnete Leistung erzielen können, indem wir Frames abfragen, die, wenn sie annotiert werden, eine große erwartete Änderung in der geschätzten Objektspur erzeugen würden. Wir implementieren einen eingeschränkten Tracker und berechnen die erwartete Änderung für putative Annotationen mit effizienten dynamischen Programmieralgorithmen.  Wir demonstrieren unser Framework anhand von vier Datensätzen, darunter zwei Benchmark-Datensätze, die mit Key-Frame-Annotationen erstellt wurden, die von Amazon Mechanical Turk erhalten wurden. Unsere Ergebnisse zeigen, dass wir äquivalente Labels für einen kleinen Bruchteil der ursprünglichen Kosten erhalten könnten.
Viele Clustering-Techniken zielen auf die Optimierung empirischer Kriterien ab, die die Form einer U-Statistik des zweiten Grades haben. Angesichts eines Maßes an Unähnlichkeit zwischen Beobachtungspaaren besteht das Ziel darin, die Punktstreuung innerhalb des Clusters über eine Klasse von Partitionen des Merkmalsraums zu minimieren. Der Zweck dieser Arbeit ist es, einen allgemeinen statistischen Rahmen zu definieren, der sich auf die Theorie der U-Prozesse stützt, um die Leistung solcher Clustering-Methoden zu untersuchen. In diesem Aufbau wird unter adäquaten Annahmen über die Komplexität der Teilmengen, die die Partitionskandidaten bilden, der Überschuss des Clustering-Risikos in der Größenordnung O(1/\sqrt{n}) nachgewiesen. Auf der Grundlage neuerer Ergebnisse in Bezug auf das Schwanzverhalten degenerierter U-Prozesse wird auch gezeigt, wie engere Ratengrenzen festgelegt werden können. Fragen der Modellauswahl, insbesondere im Zusammenhang mit der Anzahl von Clustern, die die Datenpartition bilden, werden ebenfalls berücksichtigt.
In diesem Aufsatz betrachten wir allgemeine Rank-Minimierungsprobleme, bei denen der Rang entweder in einer objektiven Funktion oder einer Randbedingung auftritt. Wir zeigen zunächst, dass eine Klasse von Matrixoptimierungsproblemen als niedrigdimensionale Vektoroptimierungsprobleme gelöst werden können. Infolgedessen stellen wir fest, dass eine Klasse von Rangminimierungsproblemen Lösungen in geschlossener Form haben. Anhand dieses Ergebnisses schlagen wir dann Strafzerlegungsmethoden für allgemeine Rangminimierungsprobleme vor. Die Konvergenzergebnisse der PD-Methoden sind in der längeren Version des Papiers gezeigt worden. Schließlich testen wir die Leistungsfähigkeit unserer Methoden, indem wir sie auf Matrizenvervollständigung und Korrelationsmatrixprobleme des nächsten niedrigen Ranges anwenden. Die Berechnungsergebnisse zeigen, dass unsere Methoden im Allgemeinen die bestehenden Methoden in Bezug auf Lösungsqualität und/oder Geschwindigkeit übertreffen.
Wir schlagen einen Algorithmus namens Sparse Manifold Clustering and Embedding (SMCE) zur gleichzeitigen Clusterbildung und Dimensionalitätsreduktion von Daten vor, die in mehreren nichtlinearen Manifolds liegen. Ähnlich wie die meisten Dimensionalitätsreduktionsmethoden findet SMCE eine kleine Nachbarschaft um jeden Datenpunkt und verbindet jeden Punkt mit seinen Nachbarn mit geeigneten Gewichten. Der Hauptunterschied besteht darin, dass SMCE sowohl die Nachbarn als auch die Gewichte automatisch findet. Dies geschieht durch die Lösung eines spärlichen Optimierungsproblems, das die Auswahl benachbarter Punkte begünstigt, die in der gleichen Mannigfaltigkeit liegen und ca.