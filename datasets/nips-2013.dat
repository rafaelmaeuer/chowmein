Wir stellen den Randomisierten AbhÃ?ngigkeitskoeffizienten (RDC) vor, ein MaÃŸ fÃ?r die nichtlineare AbhÃ?ngigkeit zwischen Zufallsvariablen beliebiger Dimension auf der Grundlage des Hirschfeld-Gebelein-RÃ©nyi-Maximum-Korrelationskoeffizienten. Der RDC wird als Korrelation von zufÃ?lligen nichtlinearen Kopula-Projektionen definiert; er ist invariant gegenÃ?ber marginalen Verteilungstransformationen, hat einen geringen Rechenaufwand und ist einfach zu implementieren: nur fÃ?nf Zeilen R-Code, der am Ende des Papiers enthalten ist.
In der Textanalyse werden Dokumente als unorganisierte Säcke von Wörtern dargestellt, Modelle von Zählmerkmalen basieren typischerweise auf dem Mischen einer kleinen Anzahl von Themen \cite{lda,sam}. In letzter Zeit wurde beobachtet, dass sich bei vielen Textkorpora Dokumente reibungslos ineinander entwickeln, wobei einige Merkmale wegfallen und neue eingeführt werden. Das Zählgitter \cite{cgUai} modelliert diese räumliche Metapher wörtlich: es ist ein mehrdimensionales Gitter von Wortverteilungen, das so gelernt wurde, dass die eigene Verteilung von Merkmalen eines Dokuments als Summe der Histogramme modelliert werden kann, die sich in einem Fenster in dem Gitter befinden. Der größte Nachteil dieser Methode besteht darin, dass es sich im Wesentlichen um eine Mischung handelt und der gesamte Inhalt von einem einzigen zusammenhängenden Bereich auf dem Gitter erzeugt wird. Dies kann insbesondere bei niedrigdimensionalen Gittern problematisch sein. In diesem Aufsatz überwinden wir dieses Problem mit dem \emph{Componential Counting Grid}, das den komponentiellen Charakter von Themenmodellen auf das grundlegende Zählungsgitter überträgt. Wir stellen auch einen generativen Kern vor, der auf der Gitterverwendung des Dokuments basiert, sowie eine Visualisierungsstrategie, die für das Verständnis großer Textkorpora nützlich ist. Wir evaluieren unseren Ansatz zur Dokumentenklassifizierung und zum multimodalen Retrieval und erhalten Ergebnisse auf dem neuesten Stand der Technik anhand von Standard-Benchmarks.
Psychophysikalische Experimente haben gezeigt, dass das Gehirn Informationen von mehreren sensorischen Hinweisen in einer nahezu Bayes'schen optimalen Weise integriert. Die vorliegende Studie schlägt einen neuartigen Mechanismus vor, um dies zu erreichen. Wir betrachten zwei reziprok miteinander verbundene Netzwerke, die die Integration von Richtungsinformationen zwischen den dorsalen medialen superior temporalen (MSTd) und den ventralen intraparietalen (VIP) Bereichen nachahmen. Jedes Netzwerk dient als lokaler Schätzer und erhält einen unabhängigen Cue, entweder den visuellen oder den vestibulären, als direkten Input für den externen Stimulus. Wir stellen fest, dass positive gegenseitige Interaktionen die Dekodierungsgenauigkeit jedes einzelnen Netzwerks verbessern können, als ob es die Bayessche Inferenz aus zwei Cues implementiert. Unser Modell erklärt erfolgreich den experimentellen Befund, dass sowohl MSTd als auch VIP eine multisensorische Bayes'sche Integration erreichen, obwohl jeder von ihnen nur einen einzigen Cue als direkten externen Input erhält. Unser Ergebnis legt nahe, dass das Gehirn eine optimale Informationsintegration bei jedem lokalen Schätzer durch die wechselseitigen Verbindungen zwischen den kortikalen Regionen distributiv umsetzen kann.
Wir präsentieren ein Maximum Margin Framework, das Daten unter Verwendung latenter Variablen clustert. Durch die Verwendung latenter Repräsentationen ermöglicht unser Rahmenwerk die Modellierung unbeobachteter Informationen, die in den Daten eingebettet sind. Wir setzen unsere Idee durch Large Margin Learning um und entwickeln einen Algorithmus für alternierende Abstiege, um das resultierende nicht-konvexe Optimierungsproblem effektiv zu lösen. Wir instantiieren unser Rahmenwerk für das Clustering mit latenter maximaler Marge mit Tag-basierten Video-Clustering-Aufgaben, bei denen jedes Video durch ein latentes Tag-Modell dargestellt wird, das das Vorhandensein oder Fehlen von Video-Tags beschreibt. Experimentelle Ergebnisse, die mit drei Standarddatensätzen erzielt wurden, zeigen, dass die vorgeschlagene Methode sowohl nicht-latentes Maximum Margin Clustering als auch herkömmliche Clustering-Ansätze übertrifft.
Wir betrachten die robuste Optimierung für polynomiale Optimierungsprobleme, bei denen der Unsicherheitssatz ein Satz von Kandidaten-Wahrscheinlichkeitsdichtefunktionen ist. Dieser Satz ist eine Kugel um eine aus Datenproben geschätzte Dichtefunktion, d.h. er ist datengetrieben und zufällig.  Polynomiale Optimierungsprobleme sind aufgrund nichtkonvexer Ziele und Randbedingungen von Natur aus hart.  Wir zeigen jedoch, dass wir durch die Verwendung von Polynom- und Histogrammdichteschätzungen Robustheit in Bezug auf Verteilungsunsicherheitssätze einführen können, ohne das Problem zu erschweren.  Wir zeigen, dass die Lösung für das verteilungsrobuste Problem die Grenze einer Sequenz von traktionsfähigen semidefiniten Programmierungsrelaxationen ist.  Wir geben auch Finite-sample-Konsistenzgarantien für die datengetriebenen Unsicherheitsmengen.  Schließlich wenden wir unser Modell und unsere Lösungsmethode auf ein Wassernetzproblem an.
Kategoriemodelle für Objekte oder Aktivitäten beruhen in der Regel auf überwachtem Lernen, das ausreichend große Trainingssätze erfordert. Der Transfer von Wissen aus bekannten Kategorien in neuartige Klassen mit keinen oder nur wenigen Labels ist jedoch weit weniger erforscht, obwohl es sich um ein übliches Szenario handelt. In dieser Arbeit erweitern wir das Transfer-Lernen mit halb-überwachtem Lernen, um nicht gekennzeichnete Instanzen von (neuartigen) Kategorien mit keinen oder nur wenigen gekennzeichneten Instanzen zu nutzen. Unser vorgeschlagener Ansatz Propagierter semantischer Transfer kombiniert drei Hauptbestandteile. Erstens übertragen wir Informationen