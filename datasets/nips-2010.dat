Wir untersuchen wiederholte Nullsummenspiele gegen einen Gegner mit einem Budget. In Anbetracht der Tatsache, dass ein Gegner eine gewisse Einschränkung in der Abfolge der von ihm gespielten Aktionen hat, überlegen wir, was die beste gemischte Strategie des Spielers mit Kenntnis dieses Budgets sein sollte. Wir zeigen, dass die Minimax-Strategie für eine allgemeine Klasse von Spielen in normaler Form tatsächlich effizient berechenbar ist und auf einer zufälligen Playout"-Technik beruht. Wir geben drei verschiedene Anwendungen dieser algorithmischen Vorlage an: eine kostensensitive "Hedge"-Einstellung, ein besonderes Problem bei metrischen Aufgabensystemen und die Gestaltung von kombinatorischen Prognosemärkten".
Clustering ist eine grundlegende Data-Mining-Aufgabe mit einer Vielzahl von Anwendungen. Es überrascht nicht, dass es viele Clustering-Algorithmen gibt. Allerdings ist das Clustering ein schlecht definiertes Problem - angesichts eines Datensatzes ist nicht klar, was ein âkorrektesâ Clustering fÃ?r diesen Satz ist. Tatsächlich kÃ¶nnen verschiedene Algorithmen fÃ?r die gleichen EingabesÃ?tze dramatisch unterschiedliche Ergebnisse liefern. Konfrontiert mit einer konkreten Clustering-Aufgabe muss ein Benutzer einen geeigneten Clustering-Algorithmus wÃ?hlen. GegenwÃ?rtig werden solche Entscheidungen hÃ?ufig sehr ad hoc, wenn nicht sogar vÃ¶llig zufÃ?llig getroffen. Angesichts der entscheidenden Auswirkung der Wahl eines Clustering-Algorithmus auf das resultierende Clustering ist dieser Zustand wirklich bedauerlich. In diesem Beitrag befassen wir uns mit der großen Forschungsherausforderung, Werkzeuge zu entwickeln, die den Benutzern helfen, fundiertere Entscheidungen zu treffen, wenn sie sich für ein Clustering-Tool für ihre Daten entscheiden. Dies ist natürlich ein sehr ehrgeiziges Unterfangen, und in diesem Papier machen wir einige erste Schritte zu diesem Ziel. Wir schlagen vor, dieses Problem durch die Destillierung abstrakter Eigenschaften des Input-Output-Verhaltens verschiedener Clustering-Paradigmen anzugehen.  In diesem Papier zeigen wir, wie abstrakte, intuitive Eigenschaften von Clustering-Funktionen zur Taxonomisierung einer Reihe populärer Clustering-Algorithmus-Paradigmen verwendet werden können. Neben der Auseinandersetzung mit deterministischen Clustering-Algorithmen schlagen wir auch ähnliche Eigenschaften für randomisierte Algorithmen vor und verwenden sie, um funktionale Unterschiede zwischen verschiedenen gängigen Implementierungen von k-Mittel-Clustering hervorzuheben. Wir untersuchen auch Beziehungen zwischen den Eigenschaften, unabhängig von einem bestimmten Algorithmus. Insbesondere verstärken wir Kleinbergs berühmtes Unmöglichkeitsergebnis, während wir gleichzeitig einen einfacheren Beweis liefern.
Viele Daten werden natürlich durch eine unbeobachtete hierarchische Struktur modelliert. In diesem Papier schlagen wir eine flexible nichtparametrische Priorität gegenüber unbekannten Datenhierarchien vor. Der Ansatz verwendet verschachtelte Stick-Breaking-Prozesse, um Bäume von unbegrenzter Breite und Tiefe zu ermöglichen, wobei die Daten an jedem Knoten leben können und unendlich austauschbar sind. Man kann unser Modell so betrachten, dass es unendliche Mischungen liefert, bei denen die Komponenten eine Abhängigkeitsstruktur aufweisen, die einer evolutionären Diffusion einen Baum hinunter entspricht. Durch die Verwendung eines Stick-Breaking-Ansatzes können wir Markov-Ketten-Monte-Carlo-Methoden anwenden, die auf Slice Sampling basieren, um Bayes'sche Inferenz durchzuführen und aus der posterioren Verteilung an Bäumen zu simulieren. Wir wenden unsere Methode auf die hierarchische Clusterung von Bildern und die thematische Modellierung von Textdaten an.
Dieses Papier beschreibt einen probabilistischen Rahmen für die Untersuchung von Assoziationen zwischen mehreren Genotypen, Biomarkern und phänotypischen Merkmalen bei Vorhandensein von Rauschen und unbeobachteten Confoundern für große genetische Studien. Der Rahmen baut auf spärlichen linearen Methoden auf, die für die Regression entwickelt und hier modifiziert wurden, um auf kausale Strukturen reicherer Netzwerke mit latenten Variablen zu schließen. Die Methode ist durch die Verwendung von Genotypen als ``Instrumente'' motiviert, um kausale Zusammenhänge zwischen phänotypischen Biomarkern und Ergebnissen herzuleiten, ohne die üblichen restriktiven Annahmen instrumenteller Variablenmethoden zu treffen. Die Methode kann für ein effektives Screening potentiell interessanter Genotyp-Phänotyp- und Biomarker-Phänotyp-Assoziationen in genomweiten Studien eingesetzt werden, was wichtige Implikationen für die Validierung von Biomarkern als mögliche Proxy-Endpunkte für klinische Studien im Frühstadium haben kann. Handelt es sich bei den Biomarkern um Gentranskripte, kann die Methode zur Feinkartierung von quantitativen Merkmalsloci (QTLs), die in genetischen Kopplungsstudien nachgewiesen werden, verwendet werden. Die Methode wird zur Untersuchung der Auswirkungen von Gentranskript-Spiegeln in der Leber auf den Plasma-HDL-Cholesterinspiegel bei einer Probe von sequenzierten Mäusen aus einem heterogenen Bestand angewandt, wobei $\sim 10^5$ genetische Instrumente und $\sim 47 \ mal 10^3$ Gentranskripte verwendet werden.
Viele statistische $M$-Schätzer basieren auf konvexen Optimierungsproblemen, die durch die gewichtete Summe einer Verlustfunktion mit einem normbasierten Regularisierer gebildet werden.  Wir analysieren die Konvergenzraten von Gradientenverfahren erster Ordnung zur Lösung solcher Probleme innerhalb eines hochdimensionalen Rahmens, der es erlaubt, die Datendimension $d$ mit der Stichprobengröße $n$ wachsen zu lassen (und möglicherweise zu überschreiten).  Diese hochdimensionale Struktur schließt die üblichen globalen Annahmen - d.h. starke Konvexitäts- und Glättungsbedingungen - aus, die der klassischen Optimierungsanalyse zugrunde liegen.  Wir definieren entsprechend eingeschränkte Versionen dieser Bedingungen und zeigen, dass sie erfüllt sind