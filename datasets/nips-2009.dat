Trotz der großen Menge an Literatur über die oberen Grenzen der Komplexität der konvexen Analyse ist überraschend wenig über die grundsätzliche Härte dieser Probleme bekannt. Die weit verbreitete Anwendung der konvexen Optimierung beim maschinellen Lernen und in der Statistik macht ein solches Verständnis entscheidend, um die grundlegenden rechnerischen Grenzen des Lernens und Schätzens zu verstehen. In diesem Beitrag untersuchen wir die Komplexität der stochastischen konvexen Optimierung in einem Orakelmodell der Berechnung. Wir verbessern die bekannten Ergebnisse und erhalten enge Minimax-Komplexitätsschätzungen für einige Funktionsklassen. Wir erörtern auch die Auswirkungen dieser Ergebnisse auf das Verständnis der inhärenten Komplexität von groß angelegten Lern- und Schätzproblemen.
Wir stellen einen Clustering-Algorithmus zur Verfügung, der das k-Mittelwert-Ziel in der One-Pass-Streaming-Einstellung annähernd optimiert. Wir machen keine Annahmen über die Daten, und unser Algorithmus ist in Bezug auf Speicher und Berechnung sehr leichtgewichtig. Diese Einstellung eignet sich für unbeaufsichtigtes Lernen auf massiven Datensätzen oder Geräten mit beschränkten Ressourcen. Die beiden Hauptbestandteile unserer theoretischen Arbeit sind: eine Ableitung eines extrem einfachen Pseudo-Approximations-Batch-Algorithmus für k-Mittelwerte, bei dem der Algorithmus mehr als k Zentren ausgeben darf (basierend auf den jüngsten k-Mittelwerten++"), und ein Streaming-Clustering-Algorithmus, bei dem Batch-Clustering-Algorithmen auf kleinen Eingaben (Anpassung im Speicher) durchgeführt und in hierarchischer Weise kombiniert werden.  Empirische Auswertungen an realen und simulierten Daten zeigen den praktischen Nutzen unserer Methode".
Die Worst-Case-Komplexität allgemeiner dezentralisierter POMDPs, die teilweise beobachtbaren stochastischen Spielen (POSGs) entsprechen, ist sehr hoch, sowohl für den kooperativen als auch für den kompetitiven Fall.  Einige Komplexitätsreduktionen wurden durch die Ausnutzung von Unabhängigkeitsbeziehungen in einigen Modellen erreicht.  Wir zeigen, dass diese Ergebnisse etwas begrenzt sind: Wenn diese Unabhängigkeitsannahmen auf sehr kleine Weise gelockert werden, kehrt die Komplexität zu der des allgemeinen Falls zurück.
Wir befassen uns mit dem Problem des Erlernens von Klassifikatoren, wenn Beobachtungen mehrere Ansichten haben, von denen einige möglicherweise nicht bei allen Beispielen beobachtet werden.  Wir gehen davon aus, dass es Funktionen zur Erzeugung von Ansichten gibt, die die fehlenden Ansichten in annähernder Weise ergänzen können.  Diese Situation entspricht zum Beispiel dem Erlernen von Textklassifikatoren aus mehrsprachigen Sammlungen, in denen Dokumente nicht in allen Sprachen verfügbar sind.  In diesem Fall können maschinelle Übersetzungssysteme (MT) verwendet werden, um jedes Dokument in die fehlenden Sprachen zu übersetzen.  Wir leiten einen Generalisierungsfehler für Klassifikatoren ab, die an Beispielen mit mehreren künstlich erzeugten Ansichten gelernt wurden. Unser Ergebnis enthüllt einen Kompromiss zwischen der Größe des Trainingssets, der Anzahl der Ansichten und der Qualität der Funktionen zur Erzeugung der Ansichten. Infolgedessen identifizieren wir Situationen, in denen es interessanter ist, mehrere Ansichten für das Lernen zu verwenden als das klassische Lernen mit nur einer Ansicht.  Eine Erweiterung dieses Rahmens ist eine natürliche Art und Weise, nicht gekennzeichnete Multi-View-Daten beim halbüberwachten Lernen zu nutzen.  Experimentelle Ergebnisse zu einer Teilmenge der Reuters RCV1/RCV2-Sammlungen untermauern unsere Ergebnisse, indem sie zeigen, dass zusätzliche, aus MT gewonnene Ansichten die Klassifizierungsleistung in den von unserem Trade-off identifizierten Fällen erheblich verbessern können.
Wir stellen ein System vor, das eine topologische Karte einer Umgebung anhand einer Abfolge von Bildern konstruiert. Dieses System umfasst einen neuartigen Bildähnlichkeitsscore, der eine dynamische Programmierung verwendet, um Bilder unter gleichzeitiger Verwendung sowohl des Aussehens als auch der relativen Positionen lokaler Merkmale abzugleichen. Zusätzlich wird eine MRF konstruiert, um die Wahrscheinlichkeit von Schleifenschlüssen zu modellieren. Eine lokal optimale Beschriftung wird mit Loopy-BP gefunden. Schließlich skizzieren wir eine Methode zur Erzeugung einer topologischen Karte aus Schleifenschlussdaten. Die Ergebnisse werden anhand von vier urbanen Sequenzen und einer Indoor-Sequenz präsentiert.
In diesem Beitrag wird das Problem der Auswahl unter mehreren linearen Schätzern bei der nichtparametrischen Regression behandelt; dazu gehören die Modellauswahl für die lineare Regression, die Wahl eines Regularisierungsparameters bei der Kernel-Ridge-Regression oder Spline-Glättung und die Wahl eines Kernels beim Lernen mit mehreren Kernen. Wir schlagen einen neuen Algorithmus vor, der zunächst die Varianz des Rauschens konsistent schätzt, basierend auf dem Konzept der minimalen Strafe, das zuvor im Zusammenhang mit der Modellauswahl eingeführt wurde. Dann wird bewiesen, dass das Stopfen unserer Varianzschätzung in Mallows $C_L$ Strafe zu einem Algorithmus führt, der eine Orakelungleichheit erfüllt. Simulationsexperimente mit Kernel-Ridge-Regression und Multiple-Kernel-Learning zeigen, dass der vorgeschlagene Algorithmus oft bestehende Kalibrierungsverfahren wie die 10-fache Kreuzvalidierung oder die verallgemeinerte Kreuzvalidierung deutlich verbessert.
Es wird ein Algorithmus zum Online-Lernen von Rotationen vorgestellt. Der vorgeschlagene Algorithmus beinhaltet matrixexponentierte Gradientenaktualisierungen und ist durch die Von-Neumann-Divergenz motiviert. Die additiven Aktualisierungen sind schiefsymmetrische Matrizen mit Spur Null, die die Lie-Algebra