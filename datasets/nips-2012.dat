Das Keypoint-Matching zwischen Bildpaaren unter Verwendung populärer Deskriptoren wie SIFT oder einer schnelleren Variante namens SURF ist das Herzstück vieler Algorithmen für die Computervision, einschließlich Erkennung, Mosaik und Struktur aus Bewegung. Für mobile Echtzeitanwendungen verwenden sehr schnelle, aber weniger genaue Deskriptoren wie BRIEF und verwandte Methoden eine Zufallsstichprobe von paarweisen Vergleichen der Pixelintensitäten in einem Bildfeld. Hier stellen wir den Locally Uniform Comparison Image Descriptor (LUCID) vor, eine einfache Beschreibungsmethode, die auf Permutationsabständen zwischen der Anordnung der Intensitäten von RGB-Werten zwischen zwei Patches basiert. LUCID ist in linearer Zeit in Bezug auf die Patch-Größe berechenbar und erfordert keine Fließkomma-Berechnung. Eine Analyse offenbart ein grundlegendes Problem, das das Potenzial von BRIEF und verwandten Ansätzen im Vergleich zu LUCID einschränkt. Experimente zeigen, dass LUCID schneller ist als BRIEF, und seine Genauigkeit ist direkt vergleichbar mit SURF, wobei es um mehr als eine Größenordnung schneller ist.
Dieses Papier stellt ein kernelbasiertes diskriminierendes Lern-Framework für Wahrscheinlichkeitsmaße vor. Anstatt sich auf große Sammlungen von vektoriellen Trainingsbeispielen zu stützen, lernt unser Rahmenwerk anhand einer Sammlung von Wahrscheinlichkeitsverteilungen, die zur sinnvollen Darstellung von Trainingsdaten konstruiert wurden. Indem wir diese Wahrscheinlichkeitsverteilungen als Mittelwert-Einbettungen im reproduzierenden Kernel-Hilbert-Raum (RKHS) darstellen, sind wir in der Lage, viele standardmäßige kernelbasierte Lerntechniken auf einfache Weise anzuwenden. Um dies zu erreichen, konstruieren wir eine Verallgemeinerung der Support-Vektor-Maschine (SVM), die als Support-Measure-Maschine (SMM) bezeichnet wird. Unsere Analysen der SMMs bieten verschiedene Einblicke in ihre Beziehung zu traditionellen SVMs. Basierend auf diesen Erkenntnissen schlagen wir eine flexible SVM (Flex-SVM) vor, die auf jedem Trainingsbeispiel unterschiedliche Kernel-Funktionen platziert. Experimentelle Ergebnisse sowohl auf synthetischen als auch auf realen Daten belegen die Wirksamkeit des von uns vorgeschlagenen Rahmens.
Angesichts der paarweisen Unähnlichkeiten zwischen den Datenpunkten betrachten wir das Problem, eine Untergruppe von Datenpunkten, Repräsentanten oder Beispiele genannt, zu finden, die die Datensammlung effizient beschreiben können. Wir formulieren das Problem als ein reihensparsames, regularisiertes Spurenminimierungsproblem, das durch konvexe Programmierung effizient gelöst werden kann. Die LÃ¶sung des vorgeschlagenen Optimierungsprogramms findet die ReprÃ?sentanten und die Wahrscheinlichkeit, dass jeder Datenpunkt mit jedem der ReprÃ?sentanten assoziiert ist. Wir erhalten den Bereich des Regularisierungsparameters, für den sich die Lösung des vorgeschlagenen Optimierungsprogramms von der Auswahl eines Repräsentanten zur Auswahl aller Datenpunkte als Repräsentanten ändert. Wenn Datenpunkte entsprechend den Unähnlichkeiten auf mehrere Cluster verteilt sind, zeigen wir, dass die Daten in jedem Cluster nur Repräsentanten aus diesem Cluster auswählen. Im Gegensatz zu metrischen Methoden erfordert unser Algorithmus nicht, dass es sich bei den paarweisen Unähnlichkeiten um Metriken handelt, und kann auf Unähnlichkeiten angewendet werden, die asymmetrisch sind oder die Dreiecksungleichung verletzen. Wir demonstrieren die Wirksamkeit des vorgeschlagenen Algorithmus sowohl bei synthetischen Daten als auch bei realen Datensätzen von Bildern und Text.
Groß angelegte $\ell_1$-regularisierte Verlustminimierungsprobleme treten in zahlreichen Anwendungen auf, wie z.B. komprimierte Abtastung und hochdimensionales überwachtes Lernen, einschließlich Klassifizierungs- und Regressionsprobleme.  Leistungsstarke Algorithmen und Implementierungen sind entscheidend für die effiziente Lösung dieser Probleme.  Aufbauend auf früheren Arbeiten zu Koordinatenabstiegs-Algorithmen für $\ell_1$-regulierte Probleme stellen wir eine neuartige Algorithmenfamilie vor, die als Block-greedy-Koordinatenabstieg bezeichnet wird und als Sonderfälle mehrere bestehende Algorithmen wie SCD, Greedy CD, Shotgun und Thread-greedy umfasst.  Wir geben eine vereinheitlichte Konvergenzanalyse für die Familie der Block-Greedy-Algorithmen.  Die Analyse legt nahe, dass die Block-Greedy-Koordinatenabstammung die Parallelität besser ausnutzen kann, wenn Merkmale so geclustert werden, dass das maximale innere Produkt zwischen Merkmalen in verschiedenen Blöcken klein ist.  Unsere theoretische Konvergenzanalyse wird durch experimentelle Ergebnisse unter Verwendung von Daten aus verschiedenen realen Anwendungen unterstützt.  Wir hoffen, dass die von uns angebotenen algorithmischen Ansätze und die Konvergenzanalyse nicht nur das Feld voranbringen, sondern die Forscher auch dazu ermutigen werden, den Designraum von Algorithmen zur Lösung großer $\ell_1$-Regularisierungsprobleme systematisch zu erforschen.
Unser zentrales Ziel ist die Quantifizierung der langfristigen Progression pädiatrischer neurologischer Erkrankungen, wie z.B. eine typische 10-15 Jahre dauernde Progression der kindlichen Dystonie. Zu diesem Zweck sind quantitative Modelle nur dann überzeugend, wenn sie mehrskalige Details liefern können, die von Neuronen-Spikes bis zur Biomechanik der Gliedmaßen reichen. Die Modelle müssen auch in Hyperzeit, d.h. deutlich schneller als in Echtzeit, ausgewertet werden, um brauchbare Vorhersagen treffen zu können. Wir haben eine Plattform mit digitaler VLSI-Hardware für Multiskalen-Hyperzeit-Emulationen der menschlichen motorischen Nervensysteme entwickelt.