Wir verwenden grafische Modelle und strukturiertes Lernen, um zu untersuchen, wie Menschen in aufeinanderfolgenden Entscheidungsfindungsaufgaben Richtlinien lernen. Studien über sequentielle Entscheidungsfindung bei Menschen finden häufig suboptimale Leistung im Vergleich zu einem idealen Akteur, der das grafische Modell kennt, das die Belohnung in der Umgebung erzeugt. Wir argumentieren, dass das Lernproblem, mit dem Menschen konfrontiert sind, auch das Lernen der Graphenstruktur für die Belohnungsgenerierung in der Umwelt umfasst. Wir formulieren das Struktur-Lernproblem mit Hilfe von Mischungen von Belohnungsmodellen und lösen das Problem der optimalen Handlungsauswahl mit Hilfe des Bayesschen Verstärkungslernens. Wir zeigen, dass das Strukturlernen bei ein- und zweiarmigen Banditenproblemen viele der qualitativen Verhaltensweisen erzeugt, die in früheren Studien als suboptimal angesehen wurden. Unser Argument wird durch die Ergebnisse von Experimenten gestützt, die zeigen, dass Menschen schnell lernen und neue Belohnungsstrukturen nutzen.
Wir stellen den Gaussian Process Density Sampler (GPDS) vor, ein austauschbares generatives Modell zur Verwendung bei der nichtparametrischen Bayes'schen Dichteschätzung. Die aus dem GPDS gezogenen Stichproben stimmen mit exakten, unabhängigen Stichproben aus einer festen Dichtefunktion überein, die eine Transformation einer Funktion ist, die zuvor aus einem Gaußschen Prozess gezogen wurde. Unsere Formulierung erlaubt es uns, mit Hilfe der Markov-Kette Monte Carlo, die Stichproben aus der posterioren Verteilung über Dichtefunktionen und aus der prädiktiven Verteilung im Datenraum liefert, eine unbekannte Dichte aus Daten abzuleiten. Wir können auch auf die Hyperparameter des Gaußschen Prozesses schließen. Wir vergleichen diese Dichtemodellierungstechnik mit mehreren bestehenden Techniken für ein Spielzeugproblem und eine Schädelrekonstruktionsaufgabe.
Wir beschreiben ein neues Content-Publishing-System, das Artikel aus einem redaktionell programmierten Pool, der häufig aktualisiert wird, auswählt, um sie einem Benutzer zur Verfügung zu stellen. Es wird jetzt auf einem großen Internet-Portal eingesetzt und wählt Artikel aus, die Hunderte von Millionen von Benutzerbesuchen pro Tag bedienen, wodurch sich die Anzahl der Benutzer-Klicks gegenüber dem ursprünglichen manuellen Ansatz, bei dem die Redakteure periodisch Artikel zur Anzeige auswählten, erheblich erhöht. Zu den Herausforderungen, mit denen wir konfrontiert sind, gehören ein dynamischer Content-Pool, eine kurze Lebensdauer der Artikel, nicht-stationäre Klickraten und ein extrem hohes Verkehrsaufkommen. Das grundlegende Problem, das wir lösen müssen, besteht darin, schnell zu ermitteln, welche Artikel beliebt sind (vielleicht innerhalb verschiedener Nutzersegmente), und diese zu nutzen, solange sie aktuell sind. Wir müssen auch den zugrundeliegenden Pool ständig untersuchen, um vielversprechende Alternativen zu identifizieren, wobei wir schlecht abschneidende Artikel schnell aussortieren müssen. Unser Ansatz basiert auf der Verfolgung der Leistung pro Artikel in nahezu Echtzeit durch Online-Modelle. Wir beschreiben die Merkmale und Einschränkungen unserer Anwendungsumgebung, diskutieren unsere Designwahl und zeigen die Bedeutung und Wirksamkeit der Kopplung von Online-Modellen mit einem einfachen Randomisierungsverfahren auf. Wir diskutieren die Herausforderungen, die in einer Produktionsumgebung für die Veröffentlichung von Online-Inhalten auftreten, und weisen auf Probleme hin, die sorgfältige Aufmerksamkeit verdienen. Unsere Analyse dieser Anwendung schlägt auch eine Reihe von zukünftigen Forschungswegen vor.
Abstrakt fehlt
Beobachtungen, die aus Messungen von Beziehungen für Objektpaare bestehen, treten in vielen Bereichen auf, z. B. bei Proteininteraktionen und genregulatorischen Netzwerken, Sammlungen von Autor-Empfänger-E-Mails und sozialen Netzwerken. Die Analyse solcher Daten mit probabilistischen Modellen kann sich als heikel erweisen, da die einfachen Austauschbarkeitsannahmen, die vielen Boilerplate-Modellen zugrunde liegen, nicht mehr zutreffen. In diesem Aufsatz beschreiben wir eine Klasse von latent variablen Modellen solcher Daten, die als stochastische Blockmodelle mit gemischter Mitgliedschaft bezeichnet werden. Dieses Modell erweitert Blockmodelle für relationale Daten um solche, die eine latente relationale Struktur mit gemischter Mitgliedschaft erfassen und so eine objektspezifische niedrigdimensionale Darstellung liefern. Wir entwickeln einen allgemeinen Variationsinferenz-Algorithmus zur schnellen approximativen posterioren Inferenz. Wir untersuchen Anwendungen auf soziale Netzwerke und Protein-Interaktionsnetzwerke.
Bestehende Ansätze zu nicht starrer Struktur aus Bewegung gehen davon aus, dass die momentane 3D-Form eines sich verformenden Objekts eine Linearkombination von Basisformen ist, die für jede Videosequenz neu geschätzt werden müssen. Im Gegensatz dazu schlagen wir vor, dass die sich entwickelnde 3D-Struktur durch eine Linearkombination von Basistrajektorien beschrieben wird. Der Hauptvorteil dieses lateralen Ansatzes besteht darin, dass wir während der Berechnung keine Basisvektoren schätzen müssen. Stattdessen zeigen wir, dass generische Basen über Trajektorien, wie z.B. die Basen der Diskreten Cosinus-Transformation (DCT), zur effektiven Beschreibung der meisten realen Bewegungen verwendet werden können. Dies führt zu einer signifikanten Reduzierung der Unbekannten und einer entsprechenden Stabilität der Schätzung. Wir berichten über die empirische Leistung, quantitativ unter Verwendung von Motion-Capture-Daten und qualitativ auf mehreren Videosequenzen, die nicht-starre Bewegungen zeigen, darunter stückweise starre Bewegungen, artikulierte Bewegungen, teilweise nicht-starre Bewegungen (wie z.B. ein Gesichtsausdruck) und hochgradig nicht-starre Bewegungen (wie z.B. eine tanzende Person).